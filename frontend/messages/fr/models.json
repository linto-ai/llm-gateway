{
  "title": "Modèles",
  "subtitle": "Gérer les modèles d'IA pour les fournisseurs",
  "createNew": "Créer un modèle",
  "newModelTitle": "Nouveau modèle",
  "editModel": "Modifier le modèle",
  "viewModel": "Voir le modèle",
  "deleteModel": "Supprimer le modèle",
  "deleteConfirm": "Êtes-vous sur de vouloir supprimer ce modèle ?",
  "deleteWarning": "Ce modèle peut etre utilise dans des variantes de service.",
  "createSuccess": "Modele créé avec succès",
  "updateSuccess": "Modele mis à jour avec succès",
  "deleteSuccess": "Modele supprimé avec succès",
  "deleteError": "Échec de la suppression du modèle",
  "emptyStateDescription": "Aucun modèle trouvé. Créez-en un pour commencer.",
  "emptyStateTitle": "Aucun modèle",
  "verifyAll": "Vérifiér tous les modèles",
  "verified": "Vérifié",
  "notVerified": "Non vérifié",
  "fields": {
    "name": "Nom du modèle",
    "displayName": "Nom d'affichage",
    "providerId": "Fournisseur",
    "organizationId": "ID d'organisation",
    "isVerified": "Etat de vérification",
    "metadata": "Metadonnees",
    "modelIdentifier": "Identifiant du modèle",
    "healthStatus": "Etat de sante",
    "contextLength": "Longueur du contexte",
    "maxGenerationLength": "Longueur max de génération",
    "tokenizer": "Tokenizer",
    "tokenizerClass": "Classe de tokenizer",
    "tokenizerName": "Nom du tokenizer",
    "huggingfaceRepo": "Depot HuggingFace",
    "securityLevel": "Niveau de sécurité",
    "deploymentName": "Nom de deploiement",
    "description": "Description",
    "bestUse": "Meilleur cas d'utilisation",
    "usageType": "Type d'utilisation",
    "systemPrompt": "Prompt système"
  },
  "placeholders": {
    "name": "gpt-4-turbo",
    "displayName": "GPT-4 Turbo",
    "huggingfaceRepo": "org/nom-modèle",
    "securityLevel": "Sélectionnez le niveau de sécurité",
    "tokenizer": "meta-llama/Llama-3.1-8B-Instruct",
    "deploymentName": "mon-deploiement",
    "description": "Entrez la description du modèle",
    "bestUse": "ex: résumé, traduction",
    "usageType": "ex: chat, completion",
    "systemPrompt": "Entrez le prompt système par défaut pour ce modèle"
  },
  "descriptions": {
    "huggingfaceRepo": "Chemin du depot HuggingFace pour la recherche du tokenizer",
    "deploymentName": "Nom de deploiement Azure ou personnalisé si different de l'identifiant du modèle",
    "bestUse": "Cas d'utilisation recommandes pour ce modèle",
    "systemPrompt": "Prompt système par défaut lors de l'utilisation de ce modèle",
    "isActive": "Activer ou désactiver ce modèle pour l'utilisation dans les services"
  },
  "health": {
    "title": "Etat de sante",
    "available": "Disponible",
    "unavailable": "Indisponible",
    "unknown": "Inconnu",
    "error": "Erreur",
    "lastChecked": "Derniere vérification",
    "neverChecked": "Jamais vérifié",
    "verifyNow": "Vérifiér maintenant",
    "verifying": "Verification en cours...",
    "verificationComplete": "Verification du modèle terminee",
    "verificationFailed": "Échec de la vérification du modèle",
    "verificationSuccess": "Modele vérifié avec succès",
    "modelUnavailable": "Le modèle est indisponible",
    "active": "Actif"
  },
  "limits": {
    "title": "Limites de tokens",
    "subtitle": "Fenetre de contexte et limités de génération pour ce modèle",
    "contextLength": "Longueur du contexte",
    "maxGeneration": "Longueur max de génération",
    "availableForInput": "Disponible pour l'entree",
    "resource": "Trouvez les limités exactes des modèles sur",
    "info": {
      "title": "Comprendre les limités de tokens",
      "contextWindow": "Fenetre de contexte",
      "contextWindowDesc": "Nombre total de tokens que le modèle peut traiter en une seule requete (entree + sortie combinees).",
      "contextWindowExample": "Ex: 128K tokens = ~300 pages de texte",
      "maxGeneration": "Limite de génération (sortie max)",
      "maxGenerationDesc": "Nombre maximum de tokens que le modèle peut générer en réponse. Toujours inférieur a la fenêtre de contexte.",
      "maxGenerationExample": "Ex: 16K tokens = ~40 pages de texte",
      "formula": "Formule: Entrée disponible = Contexte - Génération",
      "formulaExample": "Si contexte=128K et génération=16K, vous avez 112K pour l'entree"
    }
  },
  "securityLevels": {
    "secure": "Sécurisé",
    "sensitive": "Sensible",
    "insecure": "Non sécurisé",
    "CU": "Usage Confidentiel"
  },
  "tokenizer": {
    "title": "Tokenizer",
    "description": "Depot HuggingFace du tokenizer (ex: meta-llama/Llama-3.1-8B-Instruct)",
    "help": "Le tokenizer est utilise pour compter les tokens pour le découpage et l'estimation des coûts. Sélectionnez le bon tokenizer pour des resultats precis.",
    "current": "Actuel",
    "type": "Type de tokenizer",
    "encoding": "Encodage",
    "repository": "Depot HuggingFace",
    "selectRepo": "Sélectionnez un depot...",
    "customRepo": "Entrez un depot personnalisé...",
    "autoDetect": "Détection auto",
    "autoDescription": "Le système tentera de detecter le tokenizer en fonction de l'identifiant du modèle. Pour les modèles inconnus, un tokenizer par défaut (cl100k_base) sera utilise.",
    "download": "Telecharger le tokenizer",
    "downloaded": "Tokenizer telecharge avec succès",
    "downloading": "Telechargement du tokenizer...",
    "alreadyCached": "Tokenizer deja disponible localement",
    "downloadFailed": "Échec du telechargement du tokenizer",
    "enterRepo": "Entrez d'abord un depot HuggingFace",
    "gatedRepoError": "Accès refuse a ce tokenizer",
    "gatedRepoHint": "Ce modèle necessite une authentification. Configurez votre token HuggingFace dans la variable d'environnement HF_TOKEN.",
    "gatedRepoNeedsAccess": "Acceptation de licence requise",
    "gatedRepoAccessHint": "Vous devez accepter la licence du modèle sur HuggingFace avant de telecharger :",
    "retry": "Reessayer",
    "types": {
      "auto": "Détection auto",
      "tiktoken": "Tiktoken (OpenAI)",
      "huggingface": "HuggingFace"
    }
  }
}
