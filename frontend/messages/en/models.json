{
  "title": "Models",
  "subtitle": "Manage AI models for providers",
  "createNew": "Create Model",
  "newModelTitle": "New Model",
  "editModel": "Edit Model",
  "viewModel": "View Model",
  "deleteModel": "Delete Model",
  "deleteConfirm": "Are you sure you want to delete this model?",
  "deleteWarning": "This model may be used in service flavors.",
  "createSuccess": "Model created successfully",
  "updateSuccess": "Model updated successfully",
  "deleteSuccess": "Model deleted successfully",
  "deleteError": "Failed to delete model",
  "emptyStateDescription": "No models found. Create one to get started.",
  "emptyStateTitle": "No models",
  "verifyAll": "Verify All Models",
  "verified": "Verified",
  "notVerified": "Not Verified",
  "fields": {
    "name": "Model Name",
    "displayName": "Display Name",
    "providerId": "Provider",
    "organizationId": "Organization ID",
    "isVerified": "Verification Status",
    "metadata": "Metadata",
    "modelIdentifier": "Model Identifier",
    "healthStatus": "Health Status",
    "contextLength": "Context Length",
    "maxGenerationLength": "Max Generation Length",
    "tokenizer": "Tokenizer",
    "tokenizerClass": "Tokenizer Class",
    "tokenizerName": "Tokenizer Name",
    "huggingfaceRepo": "HuggingFace Repository",
    "securityLevel": "Security Level",
    "deploymentName": "Deployment Name",
    "description": "Description",
    "bestUse": "Best Use Case",
    "usageType": "Usage Type",
    "systemPrompt": "System Prompt"
  },
  "placeholders": {
    "name": "gpt-4-turbo",
    "displayName": "GPT-4 Turbo",
    "huggingfaceRepo": "org/model-name",
    "securityLevel": "Select security level",
    "tokenizer": "meta-llama/Llama-3.1-8B-Instruct",
    "deploymentName": "my-deployment",
    "description": "Enter model description",
    "bestUse": "e.g., summarization, translation",
    "usageType": "e.g., chat, completion",
    "systemPrompt": "Enter default system prompt for this model"
  },
  "descriptions": {
    "huggingfaceRepo": "HuggingFace repository path for tokenizer lookup",
    "deploymentName": "Azure or custom deployment name if different from model identifier",
    "bestUse": "Recommended use cases for this model",
    "systemPrompt": "Default system prompt when using this model",
    "isActive": "Enable or disable this model for service use"
  },
  "health": {
    "title": "Health Status",
    "available": "Available",
    "unavailable": "Unavailable",
    "unknown": "Unknown",
    "error": "Error",
    "lastChecked": "Last checked",
    "neverChecked": "Never checked",
    "verifyNow": "Verify Now",
    "verifying": "Verifying...",
    "verificationComplete": "Model verification complete",
    "verificationFailed": "Model verification failed",
    "verificationSuccess": "Model verified successfully",
    "modelUnavailable": "Model is unavailable",
    "active": "Active"
  },
  "limits": {
    "title": "Token Limits",
    "subtitle": "Context window and generation limits for this model",
    "contextLength": "Context Length",
    "maxGeneration": "Max Generation Length",
    "availableForInput": "Available for Input",
    "resource": "Find accurate model limits at",
    "info": {
      "title": "Understanding Token Limits",
      "contextWindow": "Context Window",
      "contextWindowDesc": "Total number of tokens the model can process in a single request (input + output combined).",
      "contextWindowExample": "e.g., 128K tokens = ~300 pages of text",
      "maxGeneration": "Generation Limit (Max Output)",
      "maxGenerationDesc": "Maximum number of tokens the model can generate in response. Always less than the context window.",
      "maxGenerationExample": "e.g., 16K tokens = ~40 pages of text",
      "formula": "Formula: Available Input = Context - Generation",
      "formulaExample": "If context=128K and generation=16K, you have 112K for input"
    }
  },
  "securityLevels": {
    "secure": "Secure",
    "medium": "Medium",
    "insecure": "Insecure"
  },
  "tokenizer": {
    "title": "Tokenizer",
    "description": "HuggingFace repository for the tokenizer (e.g. meta-llama/Llama-3.1-8B-Instruct)",
    "help": "The tokenizer is used to count tokens for chunking and cost estimation. Select the correct tokenizer for accurate results.",
    "current": "Current",
    "type": "Tokenizer Type",
    "encoding": "Encoding",
    "repository": "HuggingFace Repository",
    "selectRepo": "Select a repository...",
    "customRepo": "Enter custom repository...",
    "autoDetect": "Auto-detect",
    "autoDescription": "The system will attempt to detect the tokenizer based on the model identifier. For unknown models, a fallback tokenizer (cl100k_base) will be used.",
    "download": "Download tokenizer",
    "downloaded": "Tokenizer downloaded successfully",
    "downloading": "Downloading tokenizer...",
    "alreadyCached": "Tokenizer already available locally",
    "downloadFailed": "Failed to download tokenizer",
    "enterRepo": "Enter a HuggingFace repository first",
    "gatedRepoError": "Access denied to this tokenizer",
    "gatedRepoHint": "This model requires authentication. Set your HuggingFace token in the HF_TOKEN environment variable.",
    "gatedRepoNeedsAccess": "License agreement required",
    "gatedRepoAccessHint": "You must accept the model's license on HuggingFace before downloading:",
    "retry": "Retry",
    "types": {
      "auto": "Auto-detect",
      "tiktoken": "Tiktoken (OpenAI)",
      "huggingface": "HuggingFace"
    }
  }
}
