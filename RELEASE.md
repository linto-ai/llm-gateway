# 1.1.0

- Migrated backend to FastAPI with full async support and WebSocket result streaming
- Replaced local vLLM models with Exaion-hosted models for improved scalability
- Integrated Celery for async task management and status tracking
- Improved sentence parsing using spaCy, with cleaner LLM output and better error handling

# 1.0.0

- Make it work

# 0.1.0

- mixtral rolling prompt
- cra, cred summarization
